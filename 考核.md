# 一、吴恩达深度学习篇

## 1.1.简述过拟合和欠拟合的原因

### 1.出现过拟合的原因：

1.训练集所用的数据太少，使训练出来的分类器虽然在训练集上的表现较好，但却不具有足够的泛用性对未知的数据进行预测。

2.训练集和新数据的特征分布不一致。

3.训练集中存在噪音。而训练分类器中将噪音归为特征，忽略了真正有用的数据。

4.权值学习迭代次数过多，拟合了训练数据中的噪音和训练样本中没有代表性的特征。

### 2.出现欠拟合的原因：

1.数据未做归一化处理，使预处理的数据没有被限定在一定范围内，数据指标之间不具有可比性。

2.没有使用任何正则化方法。

3.使用了一个太大的batch side，降低了梯度下降的随机性，从而降低了网络的准确度。

4.使用了一个错误的学习率，

5.在最后一层使用错误的激活函数。

6.隐藏层神经元的是数量设置不正确。

7.使用的神经网络层数过多或过少。

## 1.2.简述正则化的原理

保留所有特征，在代价函数中加入正则项，降低参数的大小，使模型简单化，训练出来的模型产生的数据点不会集中产生于某个区域（即使数据点稀疏分散），逼近最优解，提高泛化能力。

## 1.3.简述梯度下降的原理

通过当前取得参数求出对应代价函数的偏导数，找到该点的梯度，沿着梯度的负方向找到下一个点的梯度，然后重复以上步骤，找到最优解，达到最小化代价函数的目的。

## 1.4.简述梯度下降的优化方法

### 1.Momentum-动量法

常用于处理非平稳梯度。Momentum-动量法是将上一次更新量的一个分量γ增加到当前的更新量中。γ一般称为动量项，对于在当前梯度点处有相同方向的维度，动量项增加；对于在该梯度点处改变方向的维度，动量项减小。以此减小频繁的波动，获得更快的收敛速度。

### 2.Nesterov-加速梯度下降法（NAG)

在Momentum-动量法的基础上进行优化，与动量法原理基本一致。区别在于NAG法一开始计算的不是下次要移动到的实际点，首先它会按照动量法得到的更新方向更新一步，然后计算该点的梯度值进行修正，得到最终的更新方向。通过作一个相当于预见性的更新，增强了算法的响应能力，提高算法的性能。

### 3.Adagrad-自适应梯度法

Adagrad-自适应梯度法是一种适应参数的梯度下降优化算法。即可在训练过程中动态调整学习率，该法的学习率是关于梯度平方和的累加项，若梯度是一直变化较大的参数，学习率下降会较快，满足高频特征使用较小学习率，反之亦然。以此实现对不同参数可以根据累计梯度平方和更新不同学习率。

Adagrad-自适应梯度法自适应参数的原理使得它的学习率的趋势是不断衰减的，在迭代过程早期衰减过快可能会导致后期收敛动力不足，且当学习率变得无穷小时，该算法将无法获得额外的信息更新。

### 4.Adadelta算法

Adadelta算法是Adagrad-自适应梯度法的一种扩展，目的是解决Adagrad学习率单调递减的缺点。Adadelta算法通过对于每个维度，用梯度平方的指数加权平均代替了至今全部梯度的平方和，避免后期收敛动力不足。而且该算法没有学习率这个超参数，通过用更新量的平方的指数加权平均来动态的代替了全局的标量的学习率。

### 5.RMSprop优化算法

RMSprop优化算法目的也是解决Adagrad法学习率下降的问题。与AdaGrad算法区别在于累积平方梯度的求法不同，在AdaGrad法基础上将这些梯度按元素平方做指数加权移动平均，即将目标函数自变量中每个元素的学习率通过按元素运算重新调整，再更新自变量。

### 6.Adam-自适应矩估计法

该法会对每一个参数计算出自适应学习率，本质是带有动量项的RMSprop优化算法的算法。Adam法会计算出一个指数衰减的历史梯度mt，即对梯度的一阶矩估计。同时还计算出一个指数衰减的历史平方梯度的平均vt，即对梯度的二阶矩估计。通过计算梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。且后Adam会通过计算经过偏差矫正后的一阶矩估计和二阶矩估计来抵消偏差（解决当mt和vt初始化为0时，初始化的步进值和衰减率都很小时，出现的参数更新过慢问题）

### 7.AdaMax优化算法

基于无穷范数的Adam的变体。对Adam法中计算指数衰减的梯度平方的平均扩展到LP范数，同时将![image-20230201161428657](C:\Users\31848\AppData\Roaming\Typora\typora-user-images\image-20230201161428657.png)扩展到![image-20230201161443553](C:\Users\31848\AppData\Roaming\Typora\typora-user-images\image-20230201161443553.png)，结果取最大值。该法用到了范数的相关性质，即无穷范数时，当p值趋于无穷时，范数具有较好的数值稳定性。且由于取最大值的计算操作，则不会出现参数更新量趋近于0的问题，因此不需要进行偏差矫正的操作。

### 8.Nadam-加速自适应矩估计法

Nadam-加速自适应矩估计法结合了Adam算法和NAG算法，将NAG算法应用于Adam公式中，用NAG动量代替经典动量。

## 1.5.简述常用的激活函数

### 1.Sigmoid函数

公式为：![image-20230201225905242](C:\Users\31848\AppData\Roaming\Typora\typora-user-images\image-20230201225905242.png)

特点：

1.将输入的连续实值变换为0和1之间的输出，因此对每个神经元的输出进行了归一化。

2.当x趋近于负无穷时，y趋近于0；当x趋近于正无穷时，y趋近于1；当x=0时，y=1/2。

3.图像为一个平滑的“S”。

缺点：

1.函数输出不是以0为中心，会降低权重更新的效率。

2.公式有指数函数，执行指数运算，会降低计算机的运行速度。

3.在深度神经网络中梯度反向传递时可能会导致梯度爆炸和梯度消失。

### 2.tanh函数

公式为：

![image-20230202165331959](C:\Users\31848\AppData\Roaming\Typora\typora-user-images\image-20230202165331959.png)

特点：

1.双曲正切函数。

2.解决了Sighmoid函数不以0为中心输出问题，其他特点与Sigmoid函数类似。

缺点：

1.梯度爆炸和梯度消失问题、指数函数运算问题依然存在。

### 3.Relu函数

公式为：

![image-20230202200033147](C:\Users\31848\AppData\Roaming\Typora\typora-user-images\image-20230202200033147.png)

特点：

1.线性整流函数，通常指代以斜坡函数及其变种为代表的非线性函数。

2.当输入值为正时，不存在梯度饱和问题。

3.因为该函数只存在线性关系，所以计算速度相比前两个函数要快。

缺点：

1.当输入值为负数时，ReLu完全失效。

2.当Relu函数的输出值为0或正数，说明此时该函数不是以0为中心的函数。

### 4.Leaky ReLU函数

公式为：

![image-20230202200747473](C:\Users\31848\AppData\Roaming\Typora\typora-user-images\image-20230202200747473.png)

特点：

1.用于解决Relu函数中出现的问题。

2.该函数通过把x的极小的先行分量给予负输入0.01x来调整Relu函数负值的零梯度问题。

3.扩大Relu函数的范围。

4.Leaky ReLU函数范围是负无穷到正无穷。

### 5.ELU(Exponential Linear Units)(Exponential Linear Units)函数

公式为：

![image-20230202201028601](C:\Users\31848\AppData\Roaming\Typora\typora-user-images\image-20230202201028601.png)

特点：

1.当输入值为负数时不会失效。函数输出的平均值接近0，以0为中心。

2.通过减少偏置偏移的影响，使正常梯度更接近于单位自然梯度，从而使均值向0加速学习。

3.当输入较小时，ELU函数会饱和至负值，从而减少前向传播的变异和信息。

4.计算强度更高。

### 6.MaxOut函数

公式为：

![image-20230202201413996](C:\Users\31848\AppData\Roaming\Typora\typora-user-images\image-20230202201413996.png)

特点：

1.是一个可学习得分段线性函数。

2.拟合能力非常强，且具有ReLU的所有优点。

缺点：

1.整体参数数量激增。

## 1.6.简述神经网络的前向传播和反向传播

### 1.简述神经网络的前向传播

神经网络的前向传播就是从输入层到输出层。前向传播从输入层开始，经过一层层的隐藏层，不断计算每一层的神经网络得到的结果后再通过激活函数的本层输出结果，最后得到输出值的过程。

### 2.简述神经网络的反向传播

神经网络的反向传播建立在梯度下降法的基础上。反向传播首先要先计算神经网络的输出值（即预测值）和实际值（对应数据的标签）的误差。然后要将计算出的误差不断地向前一层传播，梯度是按位传播，传播过程中要遵守传播梯度总和保持不变的原则。同时需要考虑到前一个神经元的权重参数，反向传播时后一层的节点会与前一层的多个节点相连，因此需要对所有节点的误差求和。目的是计算出每个神经元的误差，最后更新权重。

## 1.7.简述常用的预防过拟合的方法

1.适当减少模型学习更新时的迭代次数。

2.扩增数据集（从数据源采集更多数据、复制原有数据并加上随机噪声、重采样、根据当前数据集估计数据分布参数，使用该分布产生更多数据等）

3.在目标函数或代价函数后加上一个正则项。

4.dropout（修改神经网络本身结构）

## 1.8.简述使用全连接层结构处理图像时会出现什么问题

1.全连接层的总层数过多或者单个全连接层的神经元数过多都可能会导致模型出现过拟合的问题。

2.在检测目标物体时，尤其是目标物体是小物体。全连接层提取全图范围内的特征，待检测的目标物体可能会因为其他背景的平均特征而变得无法识别。

3.参数冗余，计算时间长，效率低。

4.空间结构的表达性不足，破坏原图的空间结构，可能会造成信息缺损。

## 1.9.简述卷积的类型，什么是卷积感受野

### 1.简述卷积的类型

#### 1.标准卷积

作用是对图像进行特征提取。

特点：

1.具有表征学习的能力

2.对输入信息进行平移不变分类，具有平移不变性

3.卷积核参数共享和层间连接的稀疏性可以减少计算量。

#### 2.深度卷积

深度可分离卷积可以先只考虑区域，然后再考虑通道，实现了通道和区域的分离。

#### 3.组卷积

会将输入数据分成多组，这种分组只是在深度上进行划分，即某几个通道编为一组。因为输出数据的改变，卷积核也需要做出同样的改变。组卷积本身就极大地减少了参数。能够增加filter之间的对角相关性，而且能够减少训练参数，减少过拟合的可能性。

#### 4.扩展卷积

针对图像语义分割问题中下采样会降低图像分辨率，丢失信息而提出的一种卷积思路。利用添加空洞扩大感受野，让原本3x3的卷积核，在相同参数量和计算量下拥有5x5或者更大的感受野，从而无需下采样。

### 2.什么是卷积感受野

定义为卷积神经网络每一层输出的特征图上的像素点映射回输入图像上的区域大小，即特征图上的一个点对应输入图上的区域，是卷积神经网络特征所能看到输入图像的区域。

# 二、CS231n篇

## 2.1.简述卷积神经网络（CNN）的原理

CNN可以直接接收并处理原始图片，然后用卷积核进行卷积操作，通过计算提取局部特征，再经过一系列的减少运算量，提高运算效率的操作，最后对计算得到的特征进行整合分类识别，输出结果。

## 2.2.简述卷积神经网络的优化方法

1.提高训练用数据的质量，可以使用数据清洗处理或者数据增强方法。

2.使用更精妙的卷积设计，如MobileNets、XNOR-Net、ShuffleNet等。

3.选用适当大小的卷积核。更大的卷积核准确率更高，但会降低训练速度，消耗更多的内存，可以使用DilatedConvolution。

4.选择合适的网络宽度与深度，宽度能使每一层学习到更加丰富的特征，选择合适的宽度能提升模型性能。深度能学习到更加复杂的表达，选择合适的深度能提升准确率。

5.选取最合适的激活函数、优化器。

## 2.3.简述卷积神经网络的常用层

1.输入层是模型对输入的图像进行预处理操作，常见预处理方法有去均值、归一化、PCA/SVD降维等。

2.卷积层是使用卷积核对图像的每一个特征进行局部感知，然后在更高层次对局部进行综合操作，从而得到全局信息。

3.激励层实际上是对卷积层的输出结果做一次非线性映射，目的在于帮助神经网络学习数据中的复杂模式。

4.池化层也称为欠采样或下采样。作用在于对图像特征降维，压缩数据和参数的数量，降低过拟合的几率，同时提高模型的容错性。

5.全连接层一般放在CNN结构中的最后，用于对抽象化的特征进行整合，对数据进行降维操作，达到对图片进行分类的目的。

## 2.4.简述卷积神经网络的常用结构

### 1.LeNet-5模型

该模型采用了交替连接的卷积层和下采样层对输入图像进行前向传导，并且最终通过全连接层输出概率分布。

优点：

1.输入图像和网络的拓补结构能很好吻合。

2.特征提取和模式分类同时进行，并同时在训练中产生。

3.权重共享可以减少网络的训练参数，使神经网络结构变得更简单，适应性更强。

缺点：

1.难以寻找到合适的大型训练集对网络进行训练以适应更为复杂的应用请求。

2.过拟合问题使得该模型的泛化能力较弱。

3.训练网络成本过高，需要较高的硬件性能支持。

### 2.AlexNet模型

针对LeNet-5模型的缺点，AlexNet模型被提出。

1.AlexNet模型在ImageNet数据上对网络进行了训练，使图片的数量和类别都大幅度超越了以往的数据集。

2.扩大网络规模，大大超越了LeNet-5模型。

3.引入了dropout层，目的使解决过度拟合训练数据的问题。

4.使用了图像转换、水平反射和补丁提取的数据增强技术。

5.AlexNet使用GPU进行训练，提高了神经网络的训练速度。

### 3.VGGNet模型

在AlexNet模型的基础上，针对卷积神经网络的深度，VGGNet模型被提出。特点是简洁、深度。

特点：

1.VGG由5层卷积层、3层全连接层、1层softmax输出层构成，层与层之间使用maxpool分开，所有隐藏层的激活单元都采用ReLU函数。

2.总的网络深度从11层到19层。

3.VGGNet模型通过降低卷积核的大小，增加卷积子层数来达到同样的性能，且能大幅度减少模型参数数量，增加非线性，提升模型性能。

### 4.ResNet模型

特点：

1.提出残差结构，并搭建超深的网络结构。

2.使用Batch Normalization加速训练

###  5.Google Inception Net模型（GoogLeNet）

特点：

1.使用了1x1的卷积核，可以跨通道组织信息，提高网络的表达能力，同时可以对输出通道进行升维和降维。

2.去除了最后的全连接层，用全局平均池化层。

3.控制了计算量和参数的同时，获得了好的分类性能。

4.使用了Inception Module，提高参数的利用率

5.nxn卷积核改为1xn和nx1卷积核，节约参数，加速运算，减轻过拟合，增加了一层非线性扩展模型表达能力
